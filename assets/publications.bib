%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/

%% Created for Duc Tien Dang Nguyen at 2017-09-19 16:00:34 +0100 


%% Saved with string encoding Unicode (UTF-8) 

@incollection{clefbook2018,
	author      = "Luca Piras and Barbara Caputo and Duc-Tien Dang-Nguyen and Michael Riegler and P\aa{} Halvorsen",
	title       = "Image retrieval evaluation in specific domains",
	publisher = {Springer Publishing Company},
	booktitle   = "CLEF@20 - Information Retrieval Evaluation in a Changing World: Lessons Learned from 20 Years of CLEF",
	year        = 2018,
	note = "(In print)"
}


@article{dangnguyen2017multimodalretrieval,
	Acmid = {3103613},
	Address = {New York, NY, USA},
	Articleno = {49},
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Giacinto, Giorgio and Boato, Giulia and De Natale, Francesco GB},
	Date-Modified = {2017-09-18 14:39:22 +0000},
	Doi = {10.1145/3103613},
	Issn = {1551-6857},
	Issue_Date = {August 2017},
	Journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
	Keywords = {Diversification, tourist attraction images retrieval},
	Month = aug,
	Number = {4},
	Numpages = {24},
	Pages = {49:1--49:24},
	Publisher = {ACM},
	Title = {{Multimodal Retrieval with Diversification and Relevance Feedback for Tourist Attraction Images}},
	Url = {http://doi.acm.org/10.1145/3103613},
	Volume = {13},
	Year = {2017},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/3103613},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/3103613}}

@article{boididou2017,
	Author = {Boididou, Christina and Middleton, Stuart~E. and Jin, Zhiwei and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Kompatsiaris, Yiannis},
	Date-Modified = {2017-09-18 15:19:16 +0000},
	Journal = {Journal of Multimedia Tools and Applications},
	Title = {A Comparative Study of Automated Approaches for Verifying Information with Multimedia Content on Twitter},
	Year = {2017}}

@article{dangnguyen2015_cgvsreal3d,
	Abstract = {Modern computer graphics technologies brought realism in computer-generated characters, making them achieve truly natural appearance. Besides traditional virtual reality applications such as avatars, games, or cinema, these synthetic characters may be used to generate realistic fakes, which may lead to improper use of the technology. This fact raises the demand for advanced tools able to discriminate real and artificial human faces in digital media. In this paper, we propose a method to distinguish between computer generated and natural faces by modeling and evaluating their dynamic behavior. Because of a 3D-model-based video analysis, the proposed technique allows identifying synthetic characters by detecting their more limited variability over time. Experimental results demonstrate the effectiveness of the proposed approach also on very challenging and realistic video sequences.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and Francesco G. B. De Natale},
	Date-Modified = {2017-09-18 15:15:03 +0000},
	Doi = {10.1109/TIFS.2015.2427778},
	Issn = {1556-6013},
	Journal = {IEEE Transactions on Information Forensics and Security},
	Keywords = {Computer generated versus natural, facial analysis, video forensics},
	Number = {8},
	Pages = {1752--1763},
	Publisher = {IEEE},
	Title = {{3D-Model-Based Video Analysis for Computer Generated Faces Identification}},
	Volume = {10},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/TIFS.2015.2427778}}

@article{boato2015exploiting,
	Abstract = {Diversification of search results allows for better and faster search, gaining knowledge about different perspectives and viewpoints on retrieved information sources. Recently various methods for diversification of image retrieval results have been proposed, mainly using textual information or techniques imported from the natural language processing domain. However, images contain much more information than their textual descriptions and the use of visual features deserves special attention in this context. Visual saliency provides information about parts of the image perceived as most important, which are instinctively targeted by humans when shooting a photo or looking at a picture. For this reason we propose to exploit such information to improve diversification of search results. To this purpose, we introduce a saliency-based method to re-rank the results of a query and we show that it can achieve significantly better performances as compared to the baseline approach. Experimental validation conducted on a number of queries applied to various datasets demonstrates the potential of the use of saliency information for the diversification of image retrieval results.},
	Author = {Boato, Giulia and Dang-Nguyen, Duc-Tien and Muratov, Oleg and Alajlan, Naif and De Natale, Francesco GB},
	Doi = {10.1007/s11042-015-2526-4},
	Issn = {1573-7721},
	Journal = {Journal of Multimedia Tools and Applications},
	Keywords = {Visual saliency, Content-based image retrieval, Diversity},
	Pages = {1--22},
	Title = {Exploiting visual saliency for increasing diversity of image retrieval results},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11042-015-2526-4}}

@article{dao2012robust,
	Abstract = {Analyzing personal photo albums for understanding the related events is an emerging trend. A reliable event recognition tool could suggest appropriate annotation of pictures, provide the context for single image classification and tagging, achieve automatic selection and summarization, ease organization and sharing of media among users. In this paper, a novel method for fast and reliable event-type classification of personal photo albums is presented. Differently from previous approaches, the proposed method does not process photos individually but as a whole, exploiting three main features, namely Saliency, Gist, and Time, to extract an event signature, which is characteristic for a specific event type. A highly challenging database containing more than 40.000 photos belonging to 19 diverse event-types was crawled from photo-sharing websites for the purpose of modeling and performance evaluation. Experimental results showed that the proposed approach meets superior classification accuracy with limited computational complexity.},
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and De Natale, Francesco G B},
	Doi = {10.1007/s11042-012-1153-6},
	Issn = {1573-7721},
	Journal = {Journal of Multimedia Tools and Applications},
	Keywords = {Gist of the scene, Saliency map, Approximate string matching, Human vision system, Personal photo albums, Holistic approach},
	Pages = {1--29},
	Title = {Robust event discovery from photo collections using Signature Image Bases (SIBs)},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1007/s11042-012-1153-6}}


@inproceedings{dangnguyen2018icmr,
	_Note = {Accepted},
	Author = {Duc-Tien Dang-Nguyen and Michael Riegler and Liting Zhou and Cathal Gurrin},
	Booktitle = {ACM International Conference on Multimedia Retrieval},
	Title = {{Challenges and Opportunities within Personal Life Archives}},
	Year = {2018}}

@inproceedings{dcu2018lsc,
	Author = {Zhou, Liting and Zaher Hinbarji and Dang-Nguyen, Duc-Tien and Cathal Gurrin},
	Booktitle = {Workshop on Lifelog Search Challenge},
	Title = {{LIFER: An Interactive Lifelog Retrieval System}},
	Year = {2018},
	Note = {Accepted}	
}


@inproceedings{dangnguyen2018lscpanel,
	Author = {Dang-Nguyen, Duc-Tien and Klaus Schoeffmann and Wolfgang Hurst},
	Booktitle = {Workshop on Lifelog Search Challenge},
	Title = {{LSC2018 Panel - Challenges of Lifelog Search And Access}},
	Year = {2018},
	Note = {Accepted}	
}

@inproceedings{dangnguyen2018usageanalytics,
	Author = {Dang-Nguyen, Duc-Tien and Manoj Kesavulu and Markus Helfert},
	Booktitle = {International Conference on Smart Cities and Green ICT Systems (SMARTGREENS)},
	Title = {{Usage Analytics: Research Directions to Discover Insights from Cloud-based Applications}},
	Year = {2018},	
}


@inproceedings{kesavulu2018usagemonitoring,
	Author = {Manoj Kesavulu and Duc-Tien Dang-Nguyen and Markus Helfert and Marija Bezbradica},
	Booktitle = {UK Academy for Information Systems International Conference},
	Title = {{An Overview of User-level Usage Monitoring in Cloud Environment}},
	Year = {2018},
	Note = {Accepted}	
}

@inproceedings{dao2018ICPRAM,
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Kasem, Asem and Tran-The, Hung},
	Booktitle = {International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
	Title = {{HealthyClassroom - A Proof-of-Concept Study for Discovering Students' Daily Moods and Classroom Emotions to Enhance a Learning-teaching Process using Heterogeneous Sensors}},
	Year = {2018},
}

@inproceedings{binh2018ICPRAM,
	Author = {Nguyen, T. Binh and Dang-Nguyen, Duc-Tien and Tien X. Dang and Thai Phat and Gurrin, Cathal},
	Booktitle = {International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
	Title = {{A Deep Learning based Food Recognition System for Lifelog Images}},
	Year = {2018},
	Note = {Accepted}	
}

@inproceedings{Gurrin2017NTCIR-Overview,
	booktitle = {Proceedings of the 13th NTCIR Conference on Evaluation of Information Access Technologies},
	title = {{Overview of NTCIR-13 Lifelog-2 Task}},
	author = {Cathal Gurrin and Hideo Joho and Frank Hopfgartner and Liting Zhou and Rashmi Gupta and Rami Albatal and Duc-Tien Dang-Nguyen},
	year = {2017},
}

@inproceedings{Zhou2017DCUteam,
	booktitle = {Proceedings of the 13th NTCIR Conference on Evaluation of Information Access Technologies},
	title = {{DCU at the NTCIR-13 Lifelog-2 Task}},
	author = {Liting Zhou and Aaron Duane and Duc-Tien Dang-Nguyen and Cathal Gurrin},
	year = {2017},
}

@inproceedings{gurrin2017LTAoverview,
	Author = {Cathal Gurrin and Xavier Giro-i-Nieto and Petia Radeva and Mariella Dimiccoli and Duc-Tien Dang-Nguyen and Hideo Joho},
	Booktitle = {ACM Workshop on Lifelogging Tools and Applications (LTA)},
	Date-Added = {2017-09-19 14:57:56 +0000},
	Date-Modified = {2017-09-19 14:59:16 +0000},
	Title = {{LTA 2017: The Second Workshop on Lifelogging Tools and Applications}},
	Year = {2017}}

@inproceedings{zhou2017baselinesearch,
	Author = {Liting Zhou and Duc-Tien Dang-Nguyen and Cathal Gurrin},
	Booktitle = {ACM Workshop on Lifelogging Tools and Applications (LTA)},
	Date-Added = {2017-09-19 10:31:53 +0000},
	Date-Modified = {2017-09-19 10:34:52 +0000},
	Title = {A Baseline Search Engine for Personal Life Archive},
	Year = {2017}}

@inproceedings{pogorelov2017MedioComparison,
	Author = {Konstantin Pogorelov and Michael Riegler and P{\aa}l Halvorsen and Carsten Griwodz and Thomas de Lange and Kristin Ranheim Randel and Sigrun Losada Eskeland and Duc-Tien Dang-Nguyen and Olga Ostroukhova and Mathias Lux and Concetto Spampinato},
	Booktitle = {MediaEval 2017 Multimedia Benchmark Workshop},
	Date-Added = {2017-09-18 15:24:06 +0000},
	Date-Modified = {2017-09-18 15:25:28 +0000},
	Month = {September 14-16},
	Title = {A Comparison of Deep Learning with Global Features for Gastrointestinal Disease Detection},
	Year = {2017}}

@inproceedings{Riegler2017MedicoTask,
	Author = {Michael Riegler and Konstantin Pogorelov and P{\aa}l Halvorsen and Carsten Griwodz and Thomas de Lange and Kristin Ranheim Randel and Sigrun Losada Eskeland and Duc-Tien Dang-Nguyen and Mathias Lux and Concetto Spampinato},
	Booktitle = {MediaEval 2017 Multimedia Benchmark Workshop},
	Date-Added = {2017-09-18 15:22:03 +0000},
	Date-Modified = {2017-09-18 15:23:22 +0000},
	Month = {September 14-16},
	Title = {Multimedia for Medicine: The Medico Task at MediaEval 2017},
	Year = {2017}}

@inproceedings{dao2017satellite,
	Author = {Minh-Son Dao and Quang-Nhat-Minh Pham and Duc-Tien Dang-Nguyen},
	Booktitle = {MediaEval 2017 Multimedia Benchmark Workshop},
	Date-Added = {2017-09-18 15:12:20 +0000},
	Date-Modified = {2017-09-18 15:23:39 +0000},
	Month = {September 14-16},
	Title = {A Domain-based Late-Fusion for Disaster Image Retrieval from Social Media},
	Year = {2017}}

@inproceedings{phan2017LDP-TOPiciap,
	Author = {Quoc-Tin Phan and Duc-Tien Dang-Nguyen and Giulia Boato and Francesco G. B. De Natale},
	Booktitle = {GIRPR International Conference on Image Analysis and Processing},
	Date-Added = {2017-09-18 14:49:48 +0000},
	Date-Modified = {2017-09-18 14:52:49 +0000},
	Title = {{Using LDP-TOP in Video-Based Spoofing Detection}},
	Year = {2017}}

@inproceedings{zhou2017LifelogOrganizers,
	Address = {Dublin, Ireland},
	Author = {Liting Zhou and Luca Piras and Michael Rieger and Giulia Boato and Duc-Tien Dang-Nguyen and Cathal Gurrin},
	Booktitle = {CLEF 2017},
	Date-Modified = {2017-09-18 15:19:26 +0000},
	Journal = {{CLEF} working notes, CEUR},
	Month = {September 11-14},
	Publisher = {CEUR-WS.org $<$http://ceur-ws.org$>$},
	Title = {Organizer Team at ImageCLEFlifelog 2017: Baseline Approaches for Lifelog Retrieval and Summarization},
	Year = {2017}}

@Inbook{Ionescu2017,
	author="Ionescu, Bogdan
	and M{\"u}ller, Henning
	and Villegas, Mauricio
	and Arenas, Helbert
	and Boato, Giulia
	and Dang-Nguyen, Duc-Tien
	and Dicente Cid, Yashin
	and Eickhoff, Carsten
	and Seco de Herrera, Alba G.
	and Gurrin, Cathal
	and Islam, Bayzidul
	and Kovalev, Vassili
	and Liauchuk, Vitali
	and Mothe, Josiane
	and Piras, Luca
	and Riegler, Michael
	and Schwall, Immanuel",
	editor="Jones, Gareth J.F.
	and Lawless, S{\'e}amus
	and Gonzalo, Julio
	and Kelly, Liadh
	and Goeuriot, Lorraine
	and Mandl, Thomas
	and Cappellato, Linda
	and Ferro, Nicola",
	title="Overview of ImageCLEF 2017: Information Extraction from Images",
	bookTitle="Experimental IR Meets Multilinguality, Multimodality, and Interaction: 8th International Conference of the CLEF Association, CLEF 2017, Dublin, Ireland, September 11--14, 2017, Proceedings",
	year="2017",
	publisher="Springer International Publishing",
	address="Cham",
	pages="315--337",
	abstract="This paper presents an overview of the ImageCLEF 2017 evaluation campaign, an event that was organized as part of the CLEF (Conference and Labs of the Evaluation Forum) labs 2017. ImageCLEF is an ongoing initiative (started in 2003) that promotes the evaluation of technologies for annotation, indexing and retrieval for providing information access to collections of images in various usage scenarios and domains. In 2017, the 15th edition of ImageCLEF, three main tasks were proposed and one pilot task: (1) a LifeLog task about searching in LifeLog data, so videos, images and other sources; (2) a caption prediction task that aims at predicting the caption of a figure from the biomedical literature based on the figure alone; (3) a tuberculosis task that aims at detecting the tuberculosis type from CT (Computed Tomography) volumes of the lung and also the drug resistance of the tuberculosis; and (4) a remote sensing pilot task that aims at predicting population density based on satellite images. The strong participation of over 150 research groups registering for the four tasks and 27 groups submitting results shows the interest in this benchmarking campaign despite the fact that all four tasks were new and had to create their own community.",
	isbn="978-3-319-65813-1",
	doi="10.1007/978-3-319-65813-1_28",
	url="https://doi.org/10.1007/978-3-319-65813-1_28"
}


@inproceedings{dangnguyen2017LifeLogTask17,
	Address = {Dublin, Ireland},
	Author = {Duc-Tien Dang-Nguyen and Luca Piras and Michael Riegler and Giulia Boato and Liting Zhou and Cathal Gurrin},
	Booktitle = {CLEF 2017},
	Date-Modified = {2017-09-18 15:17:58 +0000},
	Month = {September 11-14},
	Publisher = {CEUR-WS.org $<$http://ceur-ws.org$>$},
	Series = {{CEUR} Workshop Proceedings},
	Title = {{Overview of ImageCLEFlifelog 2017: Lifelog Retrieval and Summarization}},
	Year = {2017}}

@inproceedings{dangnguyen2017dataset,
	_Note = {Accepted},
	Abstract = {In this paper, we address the challenge of how to build a disclosed lifelog dataset by proposing the principles for building and sharing such types of data. Based on the proposed principles, we describe processes for how we built the benchmarking lifelog dataset for NTCIR-13 - Lifelog 2 tasks. Further, a list of potential applications and a framework for anonymisation are proposed and discussed.},
	Author = {Dang-Nguyen, Duc-Tien and Zhou, Liting and Gupta, Rashmi and Riegler, Michael and Gurrin, Cathal},
	Booktitle = {Content-Based Multimedia Indexing (CBMI)},
	Title = {{Building a Disclosed Lifelog Dataset: Challenges,~Principles~and~Processes}},
	Year = {2017}}

@inproceedings{ahmad2017JORD,
	_Note = {Accepted},
	Author = {Kashif Ahmad and Michael Riegler and Ans Riaz and Nicola Conci and Duc-Tien Dang-Nguyen and P{\aa}l Halvorsen},
	Booktitle = {ACM International Conference on Multimedia Retrieval},
	Date-Modified = {2017-09-18 15:15:36 +0000},
	Title = {{The JORD System - Linking Sky and Social Multimedia Data to Natural and Technological Disasters}},
	Year = {2017}}

@inproceedings{konstantin2017,
	Author = {Konstantin Pogorelov and Kristin Ranheim Randel and Carsten Griwodz and Thomas de Lange and Sigrun Losada Eskeland and Dag Johansen and Concetto Spampinato and Dang-Nguyen, Duc-Tien and Mathias Lux and Peter Thelin Schmidt and Riegler, Michael and P{\aa}l Halvorsen},
	Booktitle = {ACM Multimedia Systems},
	Date-Modified = {2017-09-19 10:33:49 +0000},
	Title = {{Kvarsir: A Multi-Class Image-Dataset for Computer Aided Gastrointestinal Disease Detection}},
	Year = {2017}}

@inproceedings{konstantin2017hollistic,
	_Note = {Accepted},
	Author = {Konstantin Pogorelov and Carsten Griwodz and Sigrun Losada and Duc-Tien Dang-Nguyen and H{\aa}kon Stensland and Thomas de Lange and Dag Johansen and Francesco De~Natale and Kristin Ranheim and Riegler, Michael and P{\aa}l Halvorsen},
	Booktitle = {ACM Multimedia Systems},
	Date-Modified = {2017-09-19 10:33:53 +0000},
	Title = {{A Holistic Multimedia System for Gastrointestinal Tract Disease Detection}},
	Year = {2017}}

@inproceedings{dao2017,
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and Riegler, Michael and Gurrin, Cathal},
	Booktitle = {International Conference on Pattern Recognition Applications and Methods (ICPRAM)},
	Date-Modified = {2017-09-19 10:34:36 +0000},
	Title = {{Smart Lifelogging: Recognizing Human~Activities using PHASOR}},
	Year = {2017}}

@inproceedings{boididou2016vmu,
	Author = {Boididou, Christina and Middleton, Stuart E and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Riegler, Michael and Boato, Giulia and Petlund, Andreas and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2016 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:13:41 +0000},
	Publisher = {CEUR-WS},
	Title = {{The VMU Participation@ Verifying Multimedia Use 2016}},
	Year = {2016}}

@misc{dangnguyen2016NIPS,
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Riegler, Michael and Gurrin, Cathal and Boato, Giulia and Harvosen, Paal},
	Booktitle = {Challenges in Machine Learning: Gaming and Education NIPS 2016 workshop},
	Date-Modified = {2017-09-18 14:41:02 +0000},
	Title = {{ImageCLEF 2017 LifeLog task}},
	Year = {2016}}

@inproceedings{boididou2016verifying,
	Author = {Boididou, Christina and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Riegler, Michael and Middleton, Stuart E. and Petlund, Andreas and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2016 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:13:50 +0000},
	Title = {Verifying multimedia use at MediaEval 2016},
	Year = {2016}}

@inproceedings{riegler2016mmsys,
	Abstract = {In this paper, we present Heimdallr, a dataset that aims to serve two different purposes. The first purpose is action recognition and pose estimation, which requires a dataset of annotated sequences of athlete skeletons. We employed a crowdsourcing platform where people around the world were asked to annotate frames and obtained more than $3000$ fully annotated frames for $42$ different sequences with a variety of poses and actions. The second purpose is an improved understanding of crowdworkers, and for this purpose, we collected over $10000$ written feedbacks from $592$ crowdworkers. 	This is valuable information for crowdsourcing researchers who explore 	algorithms for worker quality assessment. In addition to the complete dataset, we also provide the code for the application that has been used to collect the data as an open source software.},
	Author = {Michael Riegler and Duc-Tien Dang-Nguyen and B\r{a}rd Winther and Carsten Griwodz and	Konstantin Pogorelov and P\r{a}l Halvorsen},
	Booktitle = {ACM Multimedia Systems},
	Date-Modified = {2017-09-18 15:15:48 +0000},
	Title = {Heimdallr: A dataset for sport analysis},
	Year = 2016}

@inproceedings{phan2016LDP-TOP,
	Abstract = {In this paper, we propose a novel approach for face spoofing detection using the high-order Local Derivative Pattern from Three Orthogonal Planes (LDP-TOP). The proposed method is not only simple to derive and implement, but also highly efficient, since it takes into account both spatial and temporal information in different directions of subtle face movements. According to experimental results, the proposed approach outperforms state-of-the-art methods on three reference datasets, namely Idiap REPLAY-ATTACK, CASIAFASD, and MSU MFSD. Moreover, it requires only 25 video frames from each video, i.e., only one second, and thus potentially can be performed in real time even on low-cost devices.},
	Author = {Phan, Quoc-Tin and Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Conference on Image Processing},
	Date-Modified = {2017-09-18 15:16:02 +0000},
	Keywords = {Face Anti-Spoofing, Local Derivative Pattern, Video Forensics},
	Title = {Face Spoofing Detection Using LDP-TOP},
	Year = 2016}

@inproceedings{dangnguyen2015hybrid,
	Abstract = {In this paper, we present a novel method that can produce a visual description of a landmark by choosing the most diverse pictures that best describe all the details of the queried location from community-contributed datasets. The main idea of this method is to filter out non-relevant images at a first stage and then cluster the images according to textual descriptors first, and then to visual descriptors. The extraction of images from different clusters according to a measure of user's credibility, allows obtaining a reliable set of diverse and relevant images. Experimental results performed on the MediaEval 2014 ``Retrieving Diverse Social Images'' dataset show that the proposed approach can achieve very good performance outperforming state-of-art techniques.},
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Giacinto, Giorgio and Boato, Giulia and De Natale, Francesco GB},
	Booktitle = {IEEE International Conference on Multimedia and Expo (ICME)},
	Date-Modified = {2017-09-18 15:19:57 +0000},
	Doi = {10.1109/ICME.2015.7177486},
	Keywords = {Social Image Retrieval, Diversity},
	Pages = {1--6},
	Title = {A hybrid approach for retrieving diverse social images of landmarks},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICME.2015.7177486}}

@inproceedings{dangnguyen2015multimodal,
	Abstract = {In this paper, we describe our approach and its results for the MediaEval 2015 Retrieving Diverse Social Images task. The main strength of the proposed approach is its flexibility that permits to filter out irrelevant images, and to obtain a reli- able set of diverse and relevant images. This is done by first clustering similar images according to their textual descrip- tions and their visual content, and then extracting images from different clusters according to a measure of user's cred- ibility. Experimental results shown that it is stable and has little fluctuation in both single-concept and multi-concept queries.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and Natale, Francesco G and Piras, Luca and Giacinco, Giorgio and Tuveri, Franco and Angioni, Manuela},
	Booktitle = {MediaEval 2015 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:20:06 +0000},
	Title = {Multimodal-based Diversified Summarization in Social Image Retrieval},
	Volume = {1436},
	Year = {2015}}

@inproceedings{boididou2015verifying,
	Abstract = {This paper provides an overview of the Verifying Multimedia Use task that takes places as part of the 2015 MediaEval Benchmark. The task deals with the automatic detection of manipulation and misuse of Web multimedia content. Its aim is to lay the basis for a future generation of tools that could assist media professionals in the process of verifica- tion. Examples of manipulation include maliciously tamper- ing with images and videos, e.g., splicing, removal/addition of elements, while other kinds of misuse include the reposting of previously captured multimedia content in a different con- text (e.g., a new event) claiming that it was captured there. For the 2015 edition of the task, we have generated and made available a large corpus of real-world cases of images that were distributed through tweets, along with manually assigned labels regarding their use, i.e. misleading (fake) versus appropriate (real).},
	Author = {Boididou, Christina and Andreadou, Katerina and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Riegler, Michael and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2015 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:20:12 +0000},
	Title = {Verifying multimedia use at MediaEval 2015},
	Volume = {1436},
	Year = {2015}}

@inproceedings{boididou2015certh,
	Abstract = {We propose an approach that predicts whether a tweet, which is accompanied by multimedia content (image/video), is trustworthy or deceptive. We test different combinations of quality and trust-oriented features (tweet-based, user- based and forensics) in tandem with a standard classification and an agreement-retraining technique, with the goal of pre- dicting the most likely label (fake or real) for each tweet. The experiments carried out on the Verifying Multimedia Use dataset show that the best performance is achieved when using all available features in combination with the agreement-retraining method.},
	Author = {Boididou, Christina and Papadopoulos, Symeon and Dang-Nguyen, Duc-Tien and Boato, Giulia and Kompatsiaris, Yiannis},
	Booktitle = {MediaEval 2015 Multimedia Benchmark Workshop},
	Date-Modified = {2017-09-18 15:20:20 +0000},
	Title = {{The CERTH-UNITN Participation@ Verifying Multimedia Use 2015}},
	Volume = {1436},
	Year = {2015}}

@inproceedings{dangnguyen2015raise,
	Abstract = {Digital forensics is a relatively new research area which aims at authenticating digital media by detecting possible digital forgeries. Indeed, the ever increasing availability of multimedia data on the web, coupled with the great advances reached by computer graphical tools, makes the modification of an image and the creation of visually compelling forgeries an easy task for any user. This in turns creates the need of reliable tools to validate the trustworthiness of the represented information. In such a context, we present here RAISE, a large dataset of 8156 high-resolution raw images, depicting various subjects and scenarios, properly annotated and available together with accompanying metadata. Such a wide collection of untouched and diverse data is intended to become a powerful resource for, but not limited to, forensic researchers by providing a common benchmark for a fair comparison, testing and evaluation of existing and next generation forensic algorithms. In this paper we describe how RAISE has been collected and organized, discuss how digital image forensics and many other multimedia research areas may benefit of this new publicly available benchmark dataset and test a very recent forensic technique for JPEG compression detection.},
	Acmid = {2713194},
	Author = {Dang-Nguyen, Duc-Tien and Pasquini, Cecilia and Conotter, Valentina and Boato, Giulia},
	Booktitle = {ACM Multimedia Systems},
	Doi = {10.1145/2713168.2713194},
	Isbn = {978-1-4503-3351-1},
	Keywords = {Data Set, Raw Images, Benchmark, Image Forensics},
	Pages = {219--224},
	Publisher = {ACM},
	Title = {{RAISE -- A Raw Images Dataset for Digital Image Forensics}},
	Year = {2015},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2713168.2713194}}

@inproceedings{dangnguyen2014facedynamics,
	Abstract = {Digital graphics tools are nowadays capable of rendering highly photorealistic imagery, which easily puzzle our perception of reality. This poses serious ethical and legal issues, which in turn create the need for further technologies able to ensure the trustworthiness of digital media as a true representation of reality, especially when depicting humans. In this work, we propose a novel forensic technique to tackle the problem of distinguishing computer generated (CG) from real humans in videos. It exploits the temporal information inherent of a video sequence by analyzing the spatio-temporal appearance of facial expressions in both CG and real humans. Even if rendering facial expression has reached outstanding performances, CG face appearance over time still presents some underlying mechanical properties that greatly differ from the natural muscle movements of real humans. We build an efficient classifier on a set of features describing facial dynamics and spatio-temporal changes during smiling to distinguish CG from human faces. Experimental results demonstrate the effectiveness of the proposed approach.},
	Author = {Dang-Nguyen, Duc-Tien and Conotter, Valentina and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Information Forensics and Security (WIFS)},
	Doi = {10.1109/WIFS.2014.7084321},
	Pages = {161--166},
	Title = {Video forensics based on expression dynamics},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/WIFS.2014.7084321}}

@inproceedings{conotter2014deception,
	Abstract = {We present a crowdsourcing approach to tackle the challenge of collecting hard-to-find data. Our immediate need for the data arises because we are studying edited images in context online, and the way that this use impacts users' perceptions. Study of this topic cannot advance without a large, diverse data set of image/context pairs. The image in the pair should be suspected of having been edited, and the context is the place (e.g., website or social media post) in which it has been used online. Such pairs are hard to find, and could not be collected, due to techno-practical constraints, without the support of crowdsourcing. This paper describes a three-step approach to data set creation involving mining social data, applying image analysis techniques, and, finally, making use of the crowd to complete the necessary information. We close with a discussion of the potential and limitations of the data set collected.},
	Author = {Conotter, Valentina and Dang-Nguyen, Duc-Tien and Riegler, Michael and Boato, Guilia and Larson, Martha},
	Booktitle = {ACM Internaltional Workshop on Crowdsourcing for Multimedia},
	Date-Modified = {2017-09-18 15:20:38 +0000},
	Doi = {10.1145/2660114.2660120},
	Isbn = {978-1-4503-3128-9},
	Keywords = {Human Perception; Edited Images; Test design; Data Set},
	Pages = {49--52},
	Title = {A Crowdsourced Data Set of Edited Images Online},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2660114.2660120}}

@inproceedings{dangnguyen2014Deformation,
	Abstract = {Given the recent development of advanced multimedia techniques able to support the creation of realistic computer generated characters, there is the parallel need of automatic tools allowing users to verify the source of the multimedia data they are observing, thus discriminating between artificial and natural information. In this paper, we focus on video representing human beings and we propose a novel method to identify computer generated characters by analysing the evolution of the face model in chronological order. Experimental results show that photorealistic facial animations, which are usually performed following fixed patterns, can be distinguished from natural ones, which follow much more complicated and various geometric distortions.},
	Annotation = {conference},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Conference on Image Processing},
	Date-Modified = {2017-09-18 15:20:49 +0000},
	Doi = {10.1109/ICIP.2014.7026078},
	Pages = {5327--5331},
	Title = {{Revealing Synthetic Facial Animations of Realistic Characters}},
	Year = 2014,
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ICIP.2014.7026078}}

@inproceedings{dangnguyen2014mediaeval,
	Abstract = {In this paper, we describe our approach and its results for MediaEval 2014 Retrieving Diverse Social Images Task. The basic idea of our proposed method is to filter out non-relevant images at the beginning of the process and then construct a hierarchical tree which allows to cluster the images with different criteria on visual and textual features. Experimen- tal results shown that it is stable and has little fluctuation with the number of topics.},
	Author = {Dang-Nguyen, Duc-Tien and Piras, Luca and Giacinto, Giorgio and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {MediaEval 2014 Multimedia Benchmark Workshop},
	Note = {{WINNER}},
	Title = {Retrieval of Diverse Images by Pre-filtering and Hierarchical Clustering},
	Volume = {1263},
	Year = 2014}

@misc{rosani2014eventmask,
	Author = {Rosani, Andrea and Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Date-Modified = {2017-09-18 14:37:52 +0000},
	Keywords = {Event Detection, Gaming, Saliency, Photo Galleries},
	Organization = {IEEE International Conference on Acoustics, Speech and Signal Processing, Show and Tell Demo},
	Title = {EventMask: a game-based analysis of event-related saliency in photo galleries},
	Year = {2014}}

@inproceedings{conotter2014crowd,
	Abstract = {Generally, we expect images to be an honest reflection of reality. However, this assumption is undermined by the new image editing technology, which allows for easy manipulation and distortion of digital contents. Our understanding of the implications related to the use of a manipulated data is lagging behind. In this paper we propose to exploit crowdsourcing tools in order to analyze the impact of different types of manipulation on users' perceptions of deception. Our goal is to gain significant insights about how different types of manipulations impact users' perceptions and how the context in which a modified image is used influences human perception of image deceptiveness.  Through an extensive crowdsourcing user study, we aim at demonstrating that the problem of predicting user-perceived deception can be approached by automatic methods. Analysis of results collected on Amazon Mechanical Turk platform highlights how deception is related to the level of modifications applied to the image and to the context within modified pictures are used. To the best of our knowledge, this work represents the first attempt to address to the image editing debate using automatic approaches and going beyond investigation of forgeries.},
	Author = {Conotter, Valentina and Dang-Nguyen, Duc-Tien and Boato, Giulia and Menendez, Maria and Larson, Martha},
	Booktitle = {Proc. SPIE 9014, Human Vision and Electronic Imaging XIX},
	Doi = {10.1117/12.2039418},
	Pages = {90140Y},
	Title = {Assessing the impact of image manipulation on users' perceptions of deception},
	Volume = {9014},
	Year = {2014},
	Bdsk-Url-1 = {http://dx.doi.org/10.1117/12.2039418}}

@inproceedings{dangnguyen2013counterforensic,
	Abstract = {Median filtering is a well-known non linear denoising filter often used as an harmless post-processing, sometimes also employed to affect the reliability of some forensic techniques. In this work, we present a novel counter-forensic method able to conceal the characteristic traces left by median filtering. By exploiting the knowledge of features used in existing median filtering detectors, we are able to remove the characteristic footprints via suitable random pixel modification, while keeping the quality of the counter-attacked image high. Experimental results show that the proposed method is very effective, computationally efficient and competitive with other state-of-the-art techniques.},
	Author = {Dang-Nguyen, Duc-Tien and Gebru, Israel Dejene and Conotter, Valentina and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Multimedia Signal Processing (MMSP)},
	Doi = {10.1109/MMSP.2013.6659298},
	Pages = {260--265},
	Title = {Counter-forensics of median filtering},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/MMSP.2013.6659298}}

@inproceedings{rota2013exploiting,
	Abstract = {In this paper we propose a new method to infer human social interactions using typical techniques adopted in literature for visual search and information retrieval. The main piece of information we use to discriminate among different types of interactions is provided by proxemics cues acquired by a tracker, and used to distinguish between intentional and casual interactions. The proxemics information has been acquired through the analysis of two different metrics: on the one hand we observe the current distance between subjects, and on the other hand we measure the O-space synergy between subjects. The obtained values are taken at every time step over a temporal sliding window, and processed in the Discrete Fourier Transform (DFT) domain. The features are eventually merged into an unique array, and clustered using the K-means algorithm. The clusters are reorganized using a second larger temporal window into a Bag Of Words framework, so as to build the feature vector that will feed the SVM classifier.},
	Author = {Rota, Paolo and Dang-Nguyen, Duc-Tien and Conci, Nicola and Sebe, Nicu},
	Booktitle = {Proc. SPIE 8667, Multimedia Content and Mobile Devices},
	Doi = {10.1117/12.2005307},
	Pages = {86670C},
	Title = {Exploiting visual search theory to infer social interactions},
	Volume = {8667},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1117/12.2005307}}

@inproceedings{dangnguyen2012identify,
	Abstract = {Significant improvements have been recently achieved in both quality and realism of computer generated characters, which are nowadays often very difficult to be distinguished from real ones. However, generating highly realistic facial expressions is still a challenging issue, since synthetic expressions usually follow a repetitive pattern, while in natural faces the same expression is usually produced in similar but not equal ways. In this paper, we propose a method to distinguish between computer generated and natural faces based on facial expressions analysis. In particular, small variations of the facial shape models corresponding to the same expression are used as evidence of synthetic characters.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Information Forensics and Security (WIFS)},
	Doi = {10.1109/WIFS.2012.6412658},
	Note = {{BEST PAPER AWARD}},
	Pages = {252--257},
	Title = {Identify computer generated characters by analysing facial expressions variation},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/WIFS.2012.6412658}}

@inproceedings{muratov2012saliency,
	Abstract = {In this work we introduce a new application of saliency detection as a support for forensic analysis in digital images. Indeed, although there are some attempts to achieve both detection and localization of image tampering, the majority of forensics tools require hypothesis about the modified area. Since we claim that salient objects/subjects, conveying the semantic content of the image, are the regions whose integrity is more critical, we propose to combine salient object detection and forensic analysis on the corresponding output. To this aim we introduce here an improved version of a saliency map extractor based on segmentation and describe the application of a tampering detection method allowing digital composite detection. Experimental results are presented and discussed.},
	Author = {Muratov, Oleg and Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {IEEE International Symposium on Communications Control and Signal Processing (ISCCSP)},
	Doi = {10.1109/ISCCSP.2012.6217880},
	Pages = {1--5},
	Title = {Saliency detection as a support for image forensics},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/ISCCSP.2012.6217880}}

@inproceedings{dangnguyen2012discrimination,
	Abstract = {The recent development of information and communication technology has made computer software able to create highly realistic multimedia contents that can be, for human, impossible to distinguish from the natural ones. This fact leads to the need for tools and techniques that can reliably discriminate between natural and computer generated multimedia data in forensics applications. In this paper, we focus on the specific class of images containing faces, since we consider critical to be able to discriminate between photographic faces and the photorealistic ones. To this aim, we present a new geometric-based approach relying on face asymmetry information. Experimental results show that asymmetry information could be used as a hint to tackle this problem without requiring classification tools and training or combined with state-of-the-art approaches to improve their performances.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and De Natale, Francesco G B},
	Booktitle = {European Signal Processing Conference (EUSIPCO)},
	Issn = {2219-5491},
	Keywords = {Digital Image Forensics, Computer Generated Multimedia Content},
	Pages = {1234--1238},
	Title = {Discrimination Between Computer Generated and Natural Human Faces Based on Asymmetry Information},
	Year = {2012}}

@inproceedings{dangnguyen2012supervisedmodels,
	Abstract = {Nowadays, large-scale networked social media need better search technologies to achieve suitable performance. Multimodal approaches are promising technologies to improve image ranking. This is particularly true when metadata are not completely reliable, which is a rather common case as far as user annotation, time and location are concerned. In this paper, we propose to properly combine visual information with additional multi-faceted information, to define a novel multimodal similarity measure. More specifically, we combine visual features, which strongly relate to the image content, with semantic information represented by manually annotated concepts, and geo tagging, very often available in the form of object/subject location. Furthermore, we propose a supervised machine learning approach, based on Support Vector Machines (SVMs), to automatically learn optimized weights to combine the above features. The resulting models is used as a ranking function to sort the results of a multimodal query.},
	Author = {Dang-Nguyen, Duc-Tien and Boato, Giulia and Moschitti, Alessandro and De Natale, Francesco G B},
	Booktitle = {IEEE International Workshop on Content-Based Multimedia Indexing (CBMI)},
	Doi = {10.1109/CBMI.2012.6269806},
	Issn = {1949-3983},
	Pages = {1--5},
	Title = {Supervised models for multimodal image retrieval based on visual, semantic and geographic information},
	Year = {2012},
	Bdsk-Url-1 = {http://dx.doi.org/10.1109/CBMI.2012.6269806}}

@inproceedings{dao2011signature,
	Abstract = {Quick reorganizing and draft annotating personal photo albums under event scheme is an emerging trend. In this research, a method has been developed to meet such requirements using the idea of gist and mosaic art so that viewers could understand the meaning of a whole scene without paying much attention in individual details. First, given a photo album, all chronologically ordered images are normalized to a smaller size, and then mosaicked side-by-side to create a signature image representing for that album. Next, by integrating the optimized linear programming with the color descriptor of the signature image, not only the event-type of the album but also all sub-event-types of the sub-sequence photos are decided. More than 19,000 images of five varied event-types have been used to evaluate the proposed method. Experimental results show that the proposed method could detect events towards annotation and re-organization of personal photo albums with high accuracy at a rapid speed.},
	Author = {Dao, Minh-Son and Dang-Nguyen, Duc-Tien and De Natale, Francesco G B},
	Booktitle = {ACM International Conference on Multimedia},
	Doi = {10.1145/2072298.2072045},
	Isbn = {978-1-4503-0616-4},
	Keyword = {Gist, Mosaic art, Optimization Linear Programming, Event Analysis, Personal Photo Album},
	Pages = {1481--1484},
	Title = {Signature-image-based event analysis for personal photo albums},
	Year = {2011},
	Bdsk-Url-1 = {http://dx.doi.org/10.1145/2072298.2072045}}
